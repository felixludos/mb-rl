{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from common_rl_lib import *\n",
    "sys.path.append('p3')\n",
    "import gridworld as gw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Prediction Algorithms - using dynamics model\n",
    "LA and DP algorithms for solving small MDPs directly or iteratively\n",
    "from generated MDPs - to get the true value function and compare it to the sampling methods below\n",
    "\"\"\"\n",
    "class LASolver: # solve using linear algebra v = (1 - discount*dynamics)^-1*rewards\n",
    "    def __init__(self, mdp, policy=None):\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy # if None, then this is find the optimal policy\n",
    "        if self.mdp.deterministic:\n",
    "            self.det_solve()\n",
    "        else:\n",
    "            self.sto_solve()\n",
    "\n",
    "class ValueIterationSolver:\n",
    "    def __init__(self, mdp, policy=None, discount=0.9, threshold=1e-3):\n",
    "        self.mdp = mdp\n",
    "        self.policy = policy\n",
    "        self.givenPolicy = policy is not None\n",
    "        self.discount = discount\n",
    "        self.values = SafeDict()\n",
    "        self.threshold = threshold # threshold for convergence\n",
    "        self.solved = False\n",
    "        self.iterations = 0 # iterations completed\n",
    "    \n",
    "    def _improveValues(self): # internal function\n",
    "        self.iterations += 1\n",
    "        newValues = SafeDict()\n",
    "        max_delta = 0\n",
    "        for s in self.mdp.state_space():\n",
    "            if self.givenPolicy:\n",
    "                newValues[s] = self.computeQValueFromValues(s, self.policy.decide(s, self.mdp.action_space(s)))\n",
    "            else:\n",
    "                options = [ self.computeQValueFromValues(s, a) for a in self.mdp.action_space(s)]\n",
    "                if len(options): # not terminal state\n",
    "                    newValues[s] = max(options)\n",
    "            error = np.abs(self.values[s] - newValues[s])\n",
    "            if error > max_delta:\n",
    "                max_delta = error\n",
    "        self.values = newValues\n",
    "        return max_delta\n",
    "    \n",
    "    def computeQValueFromValues(self, state, action):\n",
    "        return sum([ prob * (self.mdp.getReward(state=state, action=action, newstate=sp) + (self.discount * self.values[sp])) \n",
    "                for sp, prob in self.mdp.getTransitions(state=state, action=action)])\n",
    "    \n",
    "    def slowCompute(self, state, action):\n",
    "        parts = [ prob * (self.mdp.getReward(state=state, action=action, newstate=sp) + (self.discount * self.values[sp])) \n",
    "                for sp, prob in self.mdp.getTransitions(action=action,state=state)]\n",
    "        #print '--res s:', state, 'a:', action, 'calc:', parts\n",
    "        return sum(parts)\n",
    "    \n",
    "    def iterate(self, iterations=10): # iterates regardless of whether mdp is already solved\n",
    "        delta = 0\n",
    "        for _ in range(iterations):\n",
    "            delta = self._improveValues()\n",
    "        if delta < self.threshold: self.solved = True\n",
    "        return delta # from last iteration\n",
    "    \n",
    "    def solve(self, threshold=None): # only solves in passed in threshold is smaller than solved threshold\n",
    "        if self.solved and threshold is not None and threshold >= self.threshold:\n",
    "            return -1 # mdp is already solved\n",
    "        if threshold is not None:\n",
    "            self.threshold = threshold\n",
    "            self.solved = False\n",
    "        converged = False\n",
    "        delta = 0\n",
    "        while not converged:\n",
    "            delta = self._improveValues()\n",
    "            converged = delta < self.threshold\n",
    "        self.solved = True\n",
    "        return self.iterations # total number of iterations completed\n",
    "            \n",
    "    def getValue(self, state):\n",
    "        return self.values[state]\n",
    "    \n",
    "    def getPolicy(self):\n",
    "        if not self.givenPolicy:\n",
    "            if not self.solved: self.solve()\n",
    "            self.policy = Policy(decision=self.computeActionFromValues)\n",
    "        return self.policy\n",
    "    \n",
    "    def computeActionFromValues(self, state):\n",
    "        actions = self.mdp.getPossibleActions(state)\n",
    "        if not len(actions):\n",
    "            return None\n",
    "        values = [self.computeQValueFromValues(state, a) for a in actions]\n",
    "        return max(zip(actions,values),key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridge 5\n",
      "discount 30\n",
      "book 16\n",
      "maze 23\n",
      "cliff2 17\n",
      "cliff 21\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run Value Iteration on Gridworld MDPs\"\"\"\n",
    "for name in gridWorlds.keys():\n",
    "    print name, ValueIterationSolver(GridMDP(name)).solve() # prints number of iterations until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Model Free Prediction Algorithms - for policy evaluation\n",
    "Standard Monte Carlo - only adjusts value function at the end of the episode\n",
    "TD(0) - bootstraps each step to incrementally improve value function\n",
    "TD(lambda) - bootstraps with eligibility function, to change between MC and TD\n",
    "\"\"\"\n",
    "class MC:\n",
    "    def __init__(self, policy=Policy(), alpha=0.1, discount=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.values = SafeDict()\n",
    "        self.policy = policy\n",
    "        self.episode = [] # keeps track of visited states in this episode\n",
    "        self.gain = 0\n",
    "        self.state = None\n",
    "        \n",
    "    def take_action(self, actions):\n",
    "        return self.policy.decide(self.state, actions)\n",
    "    \n",
    "    def eval_action(self, newstate, reward):\n",
    "        self.episode.append(newstate)\n",
    "        self.state = newstate\n",
    "        self.gain += reward\n",
    "        return self.gain\n",
    "    \n",
    "    def reset(self): # for MC this is where value function is improved\n",
    "        total_discount = 1\n",
    "        while len(self.episode):\n",
    "            s = self.episode.pop()\n",
    "            self.values[s] += self.alpha * (total_discount * self.gain - self.values[s])\n",
    "            total_discount *= self.discount\n",
    "        self.episode = []\n",
    "        self.gain = 0\n",
    "        self.state = None\n",
    "\n",
    "class TD:\n",
    "    def __init__(self, policy=Policy(), alpha=0.1, discount=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.values = SafeDict() # value function\n",
    "        self.state = None\n",
    "        self.policy = policy\n",
    "    \n",
    "    def take_action(self, actions): # reward is from previous action\n",
    "        return self.policy.decide(self.state, actions)\n",
    "    \n",
    "    def eval_action(self, newstate, reward):\n",
    "        if self.state is not None:\n",
    "            self.values[self.state] += self.alpha * (reward + self.discount * self.values[newstate] - self.values[self.state])\n",
    "            exp_reward = self.values[self.state]\n",
    "        self.state = newstate\n",
    "        return self.values[self.state] # return metric (eg. expected reward, loss, etc.)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = None\n",
    "\n",
    "class TD_lambda:\n",
    "    def __init__(self, policy=Policy(), alpha=0.1, discount=0.9, lmbda=0.9, threshold=1e-6):\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.lmbda = lmbda\n",
    "        self.threshold = threshold # remove insignificantly small elig. states to improve performance\n",
    "        self.values = SafeDict() # value function\n",
    "        self.policy = policy\n",
    "        self.eligibility = SafeDict()\n",
    "        self.state = None\n",
    "    \n",
    "    def take_action(self, actions): # reward is from previous action\n",
    "        return self.policy.decide(self.state, actions)\n",
    "    \n",
    "    def eval_action(self, newstate, reward):\n",
    "        if self.state is not None:\n",
    "            for s in self.eligibility.keys():\n",
    "                self.eligibility[s] *= self.discount * self.lmbda\n",
    "                if self.eligibility[s] < self.threshold: del self.eligibility[s] # remove insignificant states\n",
    "            self.eligibility[self.state] += 1\n",
    "            for s in self.eligibility.keys():\n",
    "                self.values[s] += self.alpha * self.eligibility[s] * (reward + self.discount * self.values[newstate] - self.values[self.state])\n",
    "        self.state = newstate\n",
    "        return self.values[self.state]\n",
    "    \n",
    "    def reset(self): # reset at the end of an episode\n",
    "        self.eligibility = SafeDict()\n",
    "        self.state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Run policy evaluation (prediction) \"\"\"\n",
    "learner = TD_lambda() # random policy\n",
    "watcher = MC()\n",
    "found_states = []\n",
    "episodes = 10\n",
    "printstep = episodes / 100 if episodes > 100 else 1\n",
    "steps = 20\n",
    "for i in range(1,episodes+1): # number of episodes\n",
    "    episode_len = 0\n",
    "    mdp.reset()\n",
    "    learner.reset()\n",
    "    watcher.reset()\n",
    "    if i % printstep == 0: print '\\nEpsiode', i, '\\n   ',\n",
    "    state = None\n",
    "    reward = None\n",
    "    done = len(mdp.action_space()) == 0\n",
    "    for _ in range(steps): # steps per episode\n",
    "        print mdp.state,\n",
    "        if done: break\n",
    "        state, reward, done = mdp.step(learner.take_action(mdp.action_space()))\n",
    "        learner.eval_action(state, reward)\n",
    "        watcher.eval_action(state, reward)\n",
    "        #state = state_maker(observation) # only neede if ai gym env is used\n",
    "        episode_len += 1\n",
    "    if i % printstep == 0: print '\\n--- len:', episode_len\n",
    "    found_states.append(len(learner.values))\n",
    "\n",
    "#plt.plot(found_states) # shouldn't really decrease, since policy doesn't change\n",
    "#plt.set_title('Number of States in Value Function')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print learner.values\n",
    "print watcher.values\n",
    "print mdp.dynamics\n",
    "print mdp.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Model-Free Control\n",
    "On policy: SARSA - similar to TD, but now with epsilon greedy action choices\\\n",
    "           SARSA(lambda) - similar to TD(lambda)\n",
    "Off policy: Q-learning\n",
    "\"\"\"\n",
    "class SARSA: # on policy - so policy is run while improving\n",
    "    def __init__(self, policy=None, alpha=0.1, discount=0.9, epsilon=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.explore = lambda: epsilon > np.random.rand() # epsilon-greedy - constant epsilon\n",
    "        self.Q = SafeDict() # Q function\n",
    "        self.policy = policy\n",
    "        if self.policy is None:\n",
    "            self.policy = self.getPolicy() # epsilon greedy policy\n",
    "        self.action = None\n",
    "        self.state = None # current\n",
    "    \n",
    "    def take_action(self, actions):\n",
    "        if self.action is None:\n",
    "            self.action = self.policy.decide(self.state, actions)\n",
    "        return self.action # already decided at last evaluation\n",
    "    \n",
    "    def eval_action(self, newstate, reward, newactions):\n",
    "        if self.state is not None:\n",
    "            newaction = self.policy.decide(newstate, newactions) if len(newactions) else None\n",
    "            self.Q[self.state, self.action] += self.alpha * (reward + self.discount * self.Q[newstate, newaction] - self.Q[self.state, self.action])\n",
    "            self.action = newaction\n",
    "        self.state = newstate\n",
    "        return self.Q[self.state, self.action]\n",
    "    \n",
    "    def __chooseAction(self, state): # epsilon-greedy policy\n",
    "        if not self.explore(): # exploit\n",
    "            options = [(a, self.Q[s,a]) for s,a in [key for key in self.Q.keys() if key[0]==state]]\n",
    "            if len(options):\n",
    "                return max(options, key=lambda x: x[1])[0] # argmax\n",
    "        return None # policy will pick a random action\n",
    "    \n",
    "    def getPolicy(self, epsilon=None): # updates current policy if a prior policy was given\n",
    "        if epsilon is not None:\n",
    "            self.epsilon = epsilon\n",
    "        self.policy = Policy(decision=self.__chooseAction)\n",
    "        return self.policy\n",
    "    \n",
    "    def reset(self): # reset at the end of an episode\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "\n",
    "class SARSA_lambda: # on policy - so policy is run while improving\n",
    "    def __init__(self, policy=None, alpha=0.1, discount=0.9, epsilon=0.1, lmbda=0.9, threshold=1e-6):\n",
    "        self.alpha = alpha\n",
    "        self.discount = discount\n",
    "        self.lmbda = lmbda\n",
    "        self.explore = lambda: epsilon > np.random.rand() # epsilon-greedy - constant epsilon\n",
    "        self.threshold = threshold # remove insignificantly small elig. states to improve performance\n",
    "        self.Q = SafeDict() # Q function\n",
    "        self.E = SafeDict() # eligibility\n",
    "        self.policy = policy\n",
    "        if self.policy is None:\n",
    "            self.policy = self.getPolicy() # epsilon greedy policy\n",
    "        self.action = None\n",
    "        self.state = None # current\n",
    "    \n",
    "    def take_action(self, actions):\n",
    "        if self.action is None:\n",
    "            self.action = self.policy.decide(self.state, actions)\n",
    "        return self.action # already decided at last evaluation\n",
    "    \n",
    "    def eval_action(self, newstate, reward, newactions):\n",
    "        if self.state is not None:\n",
    "            newaction = self.policy.decide(newstate, newactions) if len(newactions) else None\n",
    "            for sa in self.E.keys():\n",
    "                self.E[sa] *= self.discount * self.lmbda\n",
    "                if self.E[sa] < self.threshold: del self.E[sa] # remove insignificant states\n",
    "            self.E[self.state, self.action] += 1\n",
    "            for sa in self.E.keys():\n",
    "                self.Q[sa] += self.alpha * self.E[sa] * (reward + self.discount * self.Q[newstate, newaction] - self.Q[self.state, self.action])\n",
    "            self.action = newaction\n",
    "        self.state = newstate\n",
    "        return self.Q[self.state, self.action]\n",
    "    \n",
    "    def reset(self): # reset at the end of an episode\n",
    "        self.E = SafeDict()\n",
    "        self.state = None\n",
    "        self.prevstate = None\n",
    "        self.prevaction = None\n",
    "    \n",
    "    def __chooseAction(self, state): # epsilon-greedy policy\n",
    "        if not self.explore(): # exploit\n",
    "            options = [(a, self.Q[s,a]) for s,a in [key for key in self.Q.keys() if key[0]==state]]\n",
    "            if len(options):\n",
    "                return max(options, key=lambda x: x[1])[0] # argmax\n",
    "        return None # policy will pick a random action\n",
    "    \n",
    "    def getPolicy(self, epsilon=None): # updates current policy if a prior policy was given\n",
    "        if epsilon is not None:\n",
    "            self.epsilon = epsilon\n",
    "        self.policy = Policy(decision=self.__chooseAction)\n",
    "        return self.policy\n",
    "    \n",
    "class Q_learning: # off policy - does not choose action, merely observes\n",
    "    def __init__(self, policy=Policy(), alpha=0.1, discount=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.policy = policy\n",
    "        self.discount = discount\n",
    "        self.Q = SafeDict() # Q function\n",
    "        self.prevstate = None\n",
    "        self.prevaction = None\n",
    "    \n",
    "    def step(self, state, action, reward): # reward is from previous action into state\n",
    "        if self.prevstate is not None and self.prevaction is not None: # initial move\n",
    "            bestAction = self.__chooseAction(state)\n",
    "            if bestAction is None:\n",
    "                bestAction = action # if you don't know any better trust the action of the supervisor\n",
    "            self.Q[self.prevstate, self.prevaction] += self.alpha * (reward + self.discount\n",
    "                                                                     * self.Q[state, bestAction]\n",
    "                                                                     - self.Q[self.prevstate, self.prevaction])\n",
    "        self.prevstate = state\n",
    "        self.prevaction = action\n",
    "        return self.Q[state, action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.prevstate = None\n",
    "        self.prevaction = None\n",
    "    \n",
    "    def __chooseAction(self, state): # epsilon-greedy policy\n",
    "        options = [(a, self.Q[s,a]) for s,a in [key for key in self.Q.keys() if key[0]==state]]\n",
    "        if len(options):\n",
    "            return max(options, key=lambda x: x[1])[0] # argmax\n",
    "        return None # policy will pick a random action\n",
    "    \n",
    "    def getPolicy(self): # build exploitative policy from Q function\n",
    "        self.policy = Policy(decision=self.__chooseAction)\n",
    "        return self.policy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Reset learner and data collection\"\"\"\n",
    "learner = SARSA_lambda() # on-policy\n",
    "watcher = Q_learning() # off-policy\n",
    "test_mdp = GridMDP('cliff')\n",
    "print test_mdp.gridName\n",
    "gains = []\n",
    "completed_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Run Model-Free Control\"\"\"\n",
    "episodes = 100\n",
    "printstep = episodes / 100 if episodes > 100 else 1\n",
    "steps = 50\n",
    "for i in range(episodes): # number of episodes\n",
    "    test_mdp.reset()\n",
    "    learner.reset()\n",
    "    watcher.reset()\n",
    "    \n",
    "    if i % printstep == 0: print 'Epsiode', i+completed_episodes,\n",
    "    gain = 0\n",
    "    episode_len = 0\n",
    "    state = test_mdp.state\n",
    "    learner.state = state # initial state\n",
    "    watcher.state = state # initial state\n",
    "    reward = None\n",
    "    while True:\n",
    "    #for _ in range(steps): # steps per episode\n",
    "        #print '\\t', test_mdp.state, \n",
    "        action = learner.take_action(test_mdp.action_space())\n",
    "        #print action,\n",
    "        state, reward, done = test_mdp.step(action)\n",
    "        #print reward\n",
    "        learner.eval_action(state, reward, test_mdp.action_space())\n",
    "        watcher.step(state, action, reward)\n",
    "        #observation, reward, done, info = env.step(np.array([learner.take_action(state, reward, actions)]))\n",
    "        #state = state_maker(observation)\n",
    "        gain += reward\n",
    "        episode_len += 1\n",
    "        #print learner.Q\n",
    "        if done: break\n",
    "    if i % printstep == 0: print 'len:', episode_len, '- gain:', gain#,'\\n'\n",
    "    gains.append(gain)\n",
    "completed_episodes += episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Compare Policies that were learned\"\"\"\n",
    "plt.plot(gains) # shouldn't really decrease, since policy doesn't change\n",
    "#plt.set_title('Gain')\n",
    "plt.show()\n",
    "\n",
    "if 0:\n",
    "    #print 'Learner (SARSA)'\n",
    "    for s in test_mdp.state_space():\n",
    "        for a in test_mdp.action_space(s):\n",
    "            print 'Q(' + str(s) + ',' + str(a) + ') = l', learner.Q[s,a], 'w', watcher.Q[s,a]\n",
    "    #print '\\nWatcher (Q-learner)'\n",
    "    #for s,a in watcher.Q.keys():\n",
    "    #    print 'Q(' + str(s) + ',' + str(a) + ') = ' + str(watcher.Q[s,a])\n",
    "else:\n",
    "    l_policy = learner.getPolicy(epsilon=0)\n",
    "    w_policy = watcher.getPolicy()\n",
    "    for s in test_mdp.state_space():\n",
    "        actions = test_mdp.action_space(s)\n",
    "        if len(actions):\n",
    "            l_a = l_policy.decide(s, actions)\n",
    "            w_a = w_policy.decide(s, actions)\n",
    "            print 'state',s,'options', actions, 'l:', l_a, 'w:', w_a\n",
    "            if l_a != w_a:\n",
    "                print '\\tlearner:', [learner.Q[s,a] for a in actions]\n",
    "                print '\\twatcher', [watcher.Q[s,a] for a in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Function Approximation - based on TD but now in continuous state space using a NN to approximate Q function with discrete action space,\n",
    "Action out framework - so output is expected value of each possible action\"\"\"\n",
    "def net_maker(actions, activator=None):\n",
    "    if activator:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(3, 20),\n",
    "            activator(),\n",
    "            nn.Linear(20, 50),\n",
    "            activator(),\n",
    "            nn.Linear(50, 20),\n",
    "            activator(),\n",
    "            nn.Linear(20, len(actions)),\n",
    "            )\n",
    "    return nn.Sequential(\n",
    "            nn.Linear(3, 20),\n",
    "            nn.Linear(20, 50),\n",
    "            nn.Linear(50, 20),\n",
    "            nn.Linear(20, len(actions)), # action out\n",
    "            )\n",
    "\n",
    "class TD_net:\n",
    "    def __init__(self, actions, activator=None, alpha=0.001, discount=0.9, epsilon=0.1):\n",
    "        self.discount = discount\n",
    "        self.explore = lambda: epsilon > np.random.rand() # epsilon-greedy - constant epsilon\n",
    "        self.actions = actions # total action space\n",
    "        self.model = net_maker(actions, activator)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=alpha, momentum=0.9)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.state = None\n",
    "        self.iter_counter = 0\n",
    "    \n",
    "    def sample_actions(self, expectations, actions):\n",
    "        expectations -= expectations.min() # guarantee expectations are positive, also makes it impossible to pick worst action\n",
    "        expectations /= np.sum(expectations) # l1 normalization\n",
    "        options = zip(actions, np.cumsum(expectations))\n",
    "        sample = np.random.sample()\n",
    "        try:\n",
    "            action = [o[0] for o in options if sample < o[1]][-1]\n",
    "        except:\n",
    "            print 'expectations', np.sum(expectations), expectations, sample\n",
    "            raise Exception('Sampling failed')\n",
    "        return action\n",
    "    \n",
    "    def take_action(self, actions):\n",
    "        if self.state is None: # initial move\n",
    "            action = np.random.choice(actions)\n",
    "            return action\n",
    "        expectations = self.copy_model(self.state).data.numpy()\n",
    "        if self.explore():\n",
    "            action = self.sample_actions(expectations, actions)\n",
    "        else: # exploit\n",
    "            action = actions[expectations.argmax()] # greedy\n",
    "        return action\n",
    "    \n",
    "    def eval_action(self, newstate, reward):\n",
    "        if self.iter_counter % 200 == 0:\n",
    "            self.copy_model = copy.deepcopy(self.model) # update frozen net\n",
    "            print 'cloned'\n",
    "        self.iter_counter += 1\n",
    "        newstate = Variable(torch.Tensor(newstate))\n",
    "        if self.state is None: # initial move\n",
    "            self.state = newstate\n",
    "            return -1\n",
    "        target = Variable(reward + self.discount*self.copy_model(newstate).data) # get target from frozen net\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(self.model(self.state), target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.state = newstate\n",
    "        return loss.data[0]\n",
    "    \n",
    "    def reset_state(self): # reset at the end of an episode\n",
    "        #self.E = SafeDict()\n",
    "        self.state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Run Function Approximation Control\n",
    "Loss seems to be blowing up - not sure why, but I'm probably using pytorch incorrectly.\n",
    "\"\"\"\n",
    "actions = action_maker()\n",
    "learner = TD_net(actions, epsilon=1)# nn.PReLU)\n",
    "gains = []\n",
    "losses = []\n",
    "initial_states = []\n",
    "episodes = 1000\n",
    "printstep = episodes / 100\n",
    "steps = 50\n",
    "for i in range(episodes): # number of episodes\n",
    "    env.reset()\n",
    "    learner.reset_state()\n",
    "    if i and i % printstep == 0: print 'Epsiode', i, \n",
    "    gain, total_loss = 0, 0\n",
    "    state = None\n",
    "    for _ in range(steps): # steps per episode\n",
    "        action = learner.take_action(actions)\n",
    "        state, reward, done, info = env.step(np.array([action]))\n",
    "        state = state[np.newaxis,:]\n",
    "        loss = learner.eval_action(state, reward)\n",
    "        if loss < 0: \n",
    "            total_loss += 1 # compensate for loss flag\n",
    "            initial_states.append(state[0])\n",
    "            print state, # print initial state\n",
    "        print loss\n",
    "        gain += reward\n",
    "        total_loss += loss\n",
    "        if loss > 1000:\n",
    "            print '***Terminating episode, loss:', loss\n",
    "            break\n",
    "        if np.isnan(loss): \n",
    "            raise Exception('Loss is nan')\n",
    "    avg_loss = total_loss / steps\n",
    "    if i and i % printstep == 0: print '- gain:', gain, '- avg loss:', avg_loss\n",
    "    gains.append(gain)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "plt.plot(gains, 'g') # shouldn't really decrease, since policy doesn't change\n",
    "plt.plot(losses[::steps], 'r')\n",
    "#plt.set_title('Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inits = np.vstack(initial_states)\n",
    "losses = np.array([0] + losses)[:,np.newaxis]\n",
    "print inits.shape[0], 'episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = np.hstack([inits, losses])\n",
    "print stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
