{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import filter_env\n",
    "from ou_noise import OUNoise\n",
    "from replay_buffer import ReplayBuffer\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters/Settings\n",
    "ENV_NAME = 'Pendulum-v0' #'InvertedPendulum-v1'\n",
    "EPISODES = 100000\n",
    "TEST = 10\n",
    "TEST_STEP = 1 #100\n",
    "\n",
    "LAYER1_SIZE = 400\n",
    "LAYER2_SIZE = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "TAU = 0.001\n",
    "BATCH_SIZE = 32 #64\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "REPLAY_START_SIZE = 100 #10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCriticNet, self).__init__()\n",
    "\n",
    "        self.actor = ActorNet(state_dim, action_dim)\n",
    "        self.critic = CriticNet(state_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        action = self.actor(state)\n",
    "        value = self.critic(state, action)\n",
    "        return value\n",
    "\n",
    "    def getAction(self, state):\n",
    "        return self.actor(state)\n",
    "\n",
    "    def getValue(self, state, action=None):\n",
    "        if action is None:\n",
    "            return self.critic(state, self.actor(state))\n",
    "        return self.critic(state, action)\n",
    "\n",
    "    #def train(self): # might not be necessary\n",
    "    #    self.critic.train()\n",
    "    #    self.actor.train()\n",
    "    \n",
    "    #def eval(self): # might not be necessary\n",
    "    #    self.critic.eval()\n",
    "    #    self.actor.eval()\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticNet, self).__init__()\n",
    "\n",
    "        # make sure all params are initizialized randomly [-1/np.sqrt(dim),1/np.sqrt(dim)]\n",
    "        self.layer1 = nn.Linear(state_dim,LAYER1_SIZE)\n",
    "        self.action_layer = nn.Linear(action_dim,LAYER2_SIZE,bias=False)\n",
    "        self.layer2 = nn.Linear(LAYER1_SIZE,LAYER2_SIZE)\n",
    "        self.output_layer = nn.Linear(LAYER2_SIZE,1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.layer1(state))\n",
    "        x = F.relu(self.action_layer(action) + self.layer2(x))\n",
    "        q = self.output_layer(x)\n",
    "        return q # predicted q value of this state-action pair\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorNet, self).__init__()\n",
    "\n",
    "        # make sure all params are initizialized randomly [-1/np.sqrt(dim),1/np.sqrt(dim)]\n",
    "        self.layer0_bn = nn.BatchNorm1d(state_dim)\n",
    "        self.layer1 = nn.Linear(state_dim,LAYER1_SIZE)\n",
    "        self.layer1_bn = nn.BatchNorm1d(LAYER1_SIZE)\n",
    "        self.layer2 = nn.Linear(LAYER1_SIZE,LAYER2_SIZE)\n",
    "        self.layer2_bn = nn.BatchNorm1d(LAYER2_SIZE)\n",
    "        self.output_layer = nn.Linear(LAYER2_SIZE,action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.layer0_bn(state))\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(self.layer1_bn(x))\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(self.layer2_bn(x))\n",
    "        action = F.tanh(self.output_layer(x))\n",
    "        return action # predicted best actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-06 17:52:10,112] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True action space: [-2.], [ 2.]\n",
      "True state space: [-1. -1. -8.], [ 1.  1.  8.]\n",
      "Filtered action space: [-1.], [ 1.]\n",
      "Filtered state space: [-1. -1. -1.], [ 1.  1.  1.]\n",
      "Episode 0\n",
      "\tTraining: Average Loss: 28.2157604885\n",
      "Episode 1\n",
      "\tTraining: Average Loss: 3.87457334638\n",
      "Episode 2\n",
      "\tTraining: Average Loss: 1.16045212179\n",
      "\tTesting: Average Reward: -1208.87116199\n",
      "Episode 3\n",
      "\tTraining: Average Loss: 0.577481847033\n",
      "\tTesting: Average Reward: -1378.93442834\n",
      "Episode 4\n",
      "\tTraining: Average Loss: 0.200186561346\n",
      "\tTesting: Average Reward: -1238.68043231\n",
      "Episode 5\n",
      "\tTraining: Average Loss: 0.124796232833\n",
      "\tTesting: Average Reward: -1180.6451506\n",
      "Episode 6\n",
      "\tTraining: Average Loss: 0.123004026236\n",
      "\tTesting: Average Reward: -1351.28124717\n",
      "Episode 7\n",
      "\tTraining: Average Loss: 0.133917678036\n",
      "\tTesting: Average Reward: -1182.70054392\n",
      "Episode 8\n",
      "\tTraining: Average Loss: 0.131831472209\n",
      "\tTesting: Average Reward: -1167.19996409\n",
      "Episode 9\n",
      "\tTraining: Average Loss: 0.153557963041\n",
      "\tTesting: Average Reward: -1341.89613329\n",
      "Episode 10\n",
      "\tTraining: Average Loss: 0.17443207433\n",
      "\tTesting: Average Reward: -1244.27904375\n",
      "Episode 11\n",
      "\tTraining: Average Loss: 0.205074303024\n",
      "\tTesting: Average Reward: -1243.325491\n",
      "Episode 12\n",
      "\tTraining: Average Loss: 0.178046414424\n",
      "\tTesting: Average Reward: -1201.3902183\n",
      "Episode 13\n",
      "\tTraining: Average Loss: 0.282046123326\n",
      "\tTesting: Average Reward: -1365.03447203\n",
      "Episode 14\n",
      "\tTraining: Average Loss: 0.265232241121\n",
      "\tTesting: Average Reward: -1246.39738521\n",
      "Episode 15\n",
      "\tTraining: Average Loss: 0.305768124845\n",
      "\tTesting: Average Reward: -1324.63231309\n",
      "Episode 16\n",
      "\tTraining: Average Loss: 0.252577302791\n",
      "\tTesting: Average Reward: -1087.48156969\n",
      "Episode 17\n",
      "\tTraining: Average Loss: 0.224456657914\n",
      "\tTesting: Average Reward: -1140.94576136\n",
      "Episode 18\n",
      "\tTraining: Average Loss: 0.306064208215\n",
      "\tTesting: Average Reward: -1264.66468689\n",
      "Episode 19\n",
      "\tTraining: Average Loss: 0.206567370379\n",
      "\tTesting: Average Reward: -1232.76624313\n",
      "Episode 20\n",
      "\tTraining: Average Loss: 0.314010178347\n",
      "\tTesting: Average Reward: -1324.15658982\n",
      "Episode 21\n",
      "\tTraining: Average Loss: 0.245138319822\n",
      "\tTesting: Average Reward: -1177.55090867\n",
      "Episode 22\n",
      "\tTraining: Average Loss: 0.17001546267\n",
      "\tTesting: Average Reward: -1066.86972815\n",
      "Episode 23\n",
      "\tTraining: Average Loss: 0.230346351403\n",
      "\tTesting: Average Reward: -1261.1610475\n",
      "Episode 24\n",
      "\tTraining: Average Loss: 0.261459322169\n",
      "\tTesting: Average Reward: -1235.28382539\n",
      "Episode 25\n",
      "\tTraining: Average Loss: 0.239776631286\n",
      "\tTesting: Average Reward: -1160.71447488\n",
      "Episode 26\n",
      "\tTraining: Average Loss: 0.268104204189\n",
      "\tTesting: Average Reward: -1197.48995601\n",
      "Episode 27\n",
      "\tTraining: Average Loss: 0.230431605685\n",
      "\tTesting: Average Reward: -1258.74379499\n",
      "Episode 28\n",
      "\tTraining: Average Loss: 0.232096193726\n",
      "\tTesting: Average Reward: -1510.85814975\n",
      "Episode 29\n",
      "\tTraining: Average Loss: 0.206577821991\n",
      "\tTesting: Average Reward: -1273.06660982\n",
      "Episode 30\n",
      "\tTraining: Average Loss: 0.24031815208\n",
      "\tTesting: Average Reward: -1177.07707465\n",
      "Episode 31\n",
      "\tTraining: Average Loss: 0.329438146946\n",
      "\tTesting: Average Reward: -1126.47429387\n",
      "Episode 32\n",
      "\tTraining: Average Loss: 0.194764716122\n",
      "\tTesting: Average Reward: -1257.96928693\n",
      "Episode 33\n",
      "\tTraining: Average Loss: 0.281808725018\n",
      "\tTesting: Average Reward: -1203.62851763\n",
      "Episode 34\n",
      "\tTraining: Average Loss: 0.255664471199\n",
      "\tTesting: Average Reward: -1134.826348\n",
      "Episode 35\n",
      "\tTraining: Average Loss: 0.210868511025\n",
      "\tTesting: Average Reward: -1360.89705851\n",
      "Episode 36\n",
      "\tTraining: Average Loss: 0.297931129573\n",
      "\tTesting: Average Reward: -1224.92815254\n",
      "Episode 37\n",
      "\tTraining: Average Loss: 0.292148887927\n",
      "\tTesting: Average Reward: -1355.14721384\n",
      "Episode 38\n",
      "\tTraining: Average Loss: 0.338772211039\n",
      "\tTesting: Average Reward: -1111.25933924\n",
      "Episode 39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-faec44535c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# update targets - using exponential moving averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/torch/optim/adam.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = filter_env.makeFilteredEnv(gym.make(ENV_NAME))\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim =  env.action_space.shape[0]\n",
    "\n",
    "net = ActorCriticNet(state_dim,action_dim)\n",
    "target_net = copy.deepcopy(net)\n",
    "memory = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "noise = OUNoise(action_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "target_optim = optim.Optimizer(target_net.parameters(), {}) # to iterate over target params\n",
    "\n",
    "for episode in xrange(EPISODES):\n",
    "\n",
    "    print 'Episode', episode\n",
    "\n",
    "    net.train()\n",
    "    target_net.train() # not really necessary?\n",
    "\n",
    "    state = torch.from_numpy(env.reset().reshape(1,state_dim)).float()\n",
    "    noise.reset()\n",
    "    \n",
    "    train_steps = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    # Train\n",
    "    for step in xrange(env.spec.timestep_limit):\n",
    "\n",
    "        # Take noisy action - for exploration\n",
    "        action = net.getAction(Variable(state)).data + torch.from_numpy(noise.noise()).float()\n",
    "        #print step, 'a', action.size()\n",
    "        new_state, reward, done, _ = env.step(action.numpy().reshape((action_dim,)))\n",
    "        \n",
    "        new_state = torch.from_numpy(new_state.reshape(1,state_dim)).float()\n",
    "        reward = torch.FloatTensor([reward]).float()\n",
    "        #print step, 's', new_state.size()\n",
    "        \n",
    "        memory.add(state,action,reward,new_state,done)\n",
    "        if memory.count() > REPLAY_START_SIZE:\n",
    "            minibatch = memory.get_batch(BATCH_SIZE)\n",
    "            state_batch = torch.cat([data[0] for data in minibatch],dim=0)\n",
    "            action_batch = torch.cat([data[1] for data in minibatch],dim=0)\n",
    "            reward_batch = torch.cat([data[2] for data in minibatch])\n",
    "            next_state_batch = torch.cat([data[3] for data in minibatch])\n",
    "            done_batch = Tensor([data[4] for data in minibatch])\n",
    "            \n",
    "            # calculate y_batch from targets\n",
    "            #next_action_batch = target_net.getAction(Variable(next_state_batch))\n",
    "            value_batch = target_net.getValue(Variable(next_state_batch)).data\n",
    "            y_batch = reward_batch + GAMMA * value_batch * done_batch\n",
    "            \n",
    "            # optimize net 1 step\n",
    "            loss = criterion(net.getValue(Variable(state_batch)), Variable(y_batch))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update targets - using exponential moving averages\n",
    "            for group, target_group in zip(optimizer.param_groups, target_optim.param_groups):\n",
    "                for param, target_param in zip(group['params'], target_group['params']):\n",
    "                    target_param.data.mul_(1 - TAU)\n",
    "                    target_param.data.add_(TAU, param.data)\n",
    "                    \n",
    "            train_steps += 1\n",
    "            train_loss += loss.data[0]\n",
    "\n",
    "        state = new_state\n",
    "        if done: break\n",
    "\n",
    "    print '\\tTraining: Average Loss:', train_loss / train_steps\n",
    "\n",
    "    # Test\n",
    "    if episode % TEST_STEP == 0 and episode > TEST_STEP:\n",
    "        net.eval() # set to eval - important for batch normalization\n",
    "        total_reward = 0\n",
    "        for i in xrange(TEST):\n",
    "            state = torch.from_numpy(env.reset().reshape(1,state_dim)).float()\n",
    "            for j in xrange(env.spec.timestep_limit):\n",
    "                \n",
    "                action = net.getAction(Variable(state)).data # direct action for test\n",
    "                state, reward, done, _ = env.step(action.numpy().reshape((action_dim,)))\n",
    "                state = torch.from_numpy(state.reshape(1,state_dim)).float()\n",
    "                total_reward += reward\n",
    "                if done: break\n",
    "        ave_reward = total_reward / TEST\n",
    "        print '\\tTesting: Average Reward:', ave_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
